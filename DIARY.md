18/08/2023
i have found this documentation about mediapipe https://developers.google.com/mediapipe/solutions/vision/face_detector#get_started that i find it is quite interesting and shows quite a lot of other things that can be maybe used in the future of the project.
i have also saw the OpenCV documentation https://docs.opencv.org/4.x/ where all of the methods are explained into detail. i find it interesting that the cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) method interesting aperantly it is converting the existing view into another color space, and there is quite a bit of color spaces available. in this case the color space is RGB
did some testing where it shows if a face is detected or not. when the face is not shown the input terminal will print "none" and if there is a face the coordinates of the center of the face is shown.

19/08/2023(morning)
the face detector was detecting multiple faces now it only detects one face this was done by landmark_points[0].landmark. the x, y and z coordinates shows the center of the face, x is the width and y is the height, z is the distance from the camera but what we are interested in is the x and y coordinates only since the work we need only requires 2 dimentional space. as i only require to get the x and y dimentions. i realized that the outputs are shown between 0 and 1 (example: 0.827323317527771 0.6220095753669739) so to get the pixel values of this i am multiplying the x and y value with the frame_weight and frame_height which were gathered by frame.shape that is provided by OpenCV. the number however is in float so to turn this float, to be able to create the face circle we require to turn these float numbers into integer values. these integer values are later used in an OpenCV method (cv2.circle) to detect all the landmarks on the face, these are the parameters that cv2.circle takes so it is easier to understand cv2.circle(image, center_coordinates, radius, color, thickness). in our scenario the image we get is the frame, the center coordinates have already been found in the previous step which were (x,y) and the radius is shown as 3px since it cannot be too small or too big of circles (thickness was not used as in it made the circles to thick which comes in front of facial recognition.). here are the end results ![img_1.png](photos_of_diary/img_1.png) ![img_2.png](photos_of_diary/img_2.png)
however i still require to track the eye movement

19/08/2023(night)
after some research on https://github.com/google/mediapipe/blob/master/docs/solutions/face_mesh.md#refine_landmarks i have learned about the refine_landmarks functionality which helps refine the coordinates around the eyes or the lips. the FaceMesh model overal puts 478 3-dimensional face landmarks(CITATION https://developers.google.com/mediapipe/solutions/vision/face_landmarker/) and since we need out of all these landmarks only the iris we only get ranging from 478 to 468 shown in code as: landmarks[474:478]. the issue right now is that the camera view shows the opposite, so to fix this i have flipped the frame by using OpenCV cv2.flip(frame, 1) method to flip camera view vertically. this is what it looks like atm ![img.png](photos_of_diary/img.png)

20/08/2023
the next step is to make the cursor move with the detected eye. to do that i have used the library PyAutoGUI https://pyautogui.readthedocs.io/en/latest/ the documentation for this is quite helpful.
